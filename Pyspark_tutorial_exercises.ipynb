{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNB7EOmI1R4Jr1Xmei3I96J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PPraveen2230/PySpark---SQL-tutorial-2/blob/main/Pyspark_tutorial_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### how to find unique values in a Column"
      ],
      "metadata": {
        "id": "-F4zR6oXeHPg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLECqCDhZ-pV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHmhlBvQaZR6",
        "outputId": "3e10a788-f7f3-4cd0-e9b8-b4c4b4437f60"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "#define data\n",
        "data = [['A', 'East', 11],\n",
        "        ['A', 'East', 8],\n",
        "        ['A', 'East', 10],\n",
        "        ['B', 'West', 6],\n",
        "        ['B', 'West', 9],\n",
        "        ['C', 'East', 5]]\n",
        "\n",
        " #define column name\n",
        "columns = ['team', 'conference', 'points']\n",
        "\n",
        " # create Dataframe using data and ccolumn names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        " # view DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW8fcTafaPyP",
        "outputId": "81403db2-af9e-4333-b689-cdaa50b9d0ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+\n",
            "|team|conference|points|\n",
            "+----+----------+------+\n",
            "|   A|      East|    11|\n",
            "|   A|      East|     8|\n",
            "|   A|      East|    10|\n",
            "|   B|      West|     6|\n",
            "|   B|      West|     9|\n",
            "|   C|      East|     5|\n",
            "+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Find Unique Values in a Column\n",
        "\n",
        "df.select('team').distinct().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTb_C5ZiaXtJ",
        "outputId": "3d108b24-49ee-44f4-ae9a-b96ee461c699"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|team|\n",
            "+----+\n",
            "|   A|\n",
            "|   B|\n",
            "|   C|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Find and Sort Unique Values in a Column\n",
        "\n",
        "df.select('points').distinct().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M5L6-SQb-PK",
        "outputId": "5b9f6701-f710-4c7f-dd61-9ff35a3b79af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|points|\n",
            "+------+\n",
            "|    10|\n",
            "|     8|\n",
            "|    11|\n",
            "|     6|\n",
            "|     9|\n",
            "|     5|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find unique values in points column\n",
        "df_points = df.select('points').distinct()\n",
        "\n",
        "\n",
        "#display unique values in ascending order\n",
        "df_points.orderBy('points').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Urye-eWxcG03",
        "outputId": "a7637e00-67b1-46fe-ee2f-dc49b94f4bef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|points|\n",
            "+------+\n",
            "|     5|\n",
            "|     6|\n",
            "|     8|\n",
            "|     9|\n",
            "|    10|\n",
            "|    11|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find unique values in points column\n",
        "df_points = df.select('points').distinct()\n",
        "\n",
        "#display unique values in descending order\n",
        "df_points.orderBy('points', ascending=False).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O35vW-TcPu2",
        "outputId": "a6a0c850-d60e-48b4-d5bc-5e11fd93c764"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|points|\n",
            "+------+\n",
            "|    11|\n",
            "|    10|\n",
            "|     9|\n",
            "|     8|\n",
            "|     6|\n",
            "|     5|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Find and Count Unique Values in a Column\n",
        "\n",
        "df.groupBy('team').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmyRIXbGcbt-",
        "outputId": "f6020563-0105-4b3f-c38d-e2b9fd7ac258"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|team|count|\n",
            "+----+-----+\n",
            "|   A|    3|\n",
            "|   B|    2|\n",
            "|   C|    1|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to Select Rows by Index in DataFrame"
      ],
      "metadata": {
        "id": "1Apm9RFFeOis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Select Rows by Index in PySpark DataFrame\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYkIo14yciyE",
        "outputId": "266b88c1-fa97-4e09-c2df-d312cf231111"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+\n",
            "|team|conference|points|\n",
            "+----+----------+------+\n",
            "|   A|      East|    11|\n",
            "|   A|      East|     8|\n",
            "|   A|      East|    10|\n",
            "|   B|      West|     6|\n",
            "|   B|      West|     9|\n",
            "|   C|      East|     5|\n",
            "+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import row_number, lit\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "#add column called 'id' that contains row number from 1 to n\n",
        "w= Window().orderBy(lit ('A'))\n",
        "df= df.withColumn('id', row_number().over(w))\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9ShU1Bccx1u",
        "outputId": "1a915b67-2041-4288-a30d-e9a7e1b00a5b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+---+\n",
            "|team|conference|points| id|\n",
            "+----+----------+------+---+\n",
            "|   A|      East|    11|  1|\n",
            "|   A|      East|     8|  2|\n",
            "|   A|      East|    10|  3|\n",
            "|   B|      West|     6|  4|\n",
            "|   B|      West|     9|  5|\n",
            "|   C|      East|     5|  6|\n",
            "+----+----------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# select all rows between index values 2 and 5\n",
        "df.where(col('id').between(2, 5)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16_KowVCdR_b",
        "outputId": "23df7357-f6e5-4001-9c86-2b048401eb23"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+---+\n",
            "|team|conference|points| id|\n",
            "+----+----------+------+---+\n",
            "|   A|      East|     8|  2|\n",
            "|   A|      East|    10|  3|\n",
            "|   B|      West|     6|  4|\n",
            "|   B|      West|     9|  5|\n",
            "+----+----------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find unique values in points column\n",
        "df.filter(df.id.isin(1,5,6)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNMDWlZkdnq8",
        "outputId": "a3bff591-5ff3-40ad-ceba-379fb1dc44d9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+---+\n",
            "|team|conference|points| id|\n",
            "+----+----------+------+---+\n",
            "|   A|      East|    11|  1|\n",
            "|   B|      West|     9|  5|\n",
            "|   C|      East|     5|  6|\n",
            "+----+----------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Select Columns by Index in\n",
        "DataFrame"
      ],
      "metadata": {
        "id": "aTuRBTAfeCcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#select first column in DataFrame\n",
        "df.select(df.columns[0]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1UeMzeTd2PN",
        "outputId": "d3d60ccb-228e-438c-ce5b-dd2a155895f0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|team|\n",
            "+----+\n",
            "|   A|\n",
            "|   A|\n",
            "|   A|\n",
            "|   B|\n",
            "|   B|\n",
            "|   C|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select all columns except first column in DataFrame\n",
        "df.drop(df.columns[0]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KvWKegKeSjI",
        "outputId": "8863b598-d523-43ed-e6e9-13ad0d8c9385"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+---+\n",
            "|conference|points| id|\n",
            "+----------+------+---+\n",
            "|      East|    11|  1|\n",
            "|      East|     8|  2|\n",
            "|      East|    10|  3|\n",
            "|      West|     6|  4|\n",
            "|      West|     9|  5|\n",
            "|      East|     5|  6|\n",
            "+----------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select all columns between index 0 and 2, not including 2\n",
        "df.select(df.columns[0:2]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk0axi3teV0H",
        "outputId": "99df5f0d-c4b4-4b32-db1f-bcc1669bbf38"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+\n",
            "|team|conference|\n",
            "+----+----------+\n",
            "|   A|      East|\n",
            "|   A|      East|\n",
            "|   A|      East|\n",
            "|   B|      West|\n",
            "|   B|      West|\n",
            "|   C|      East|\n",
            "+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Select Specific Column by Index\n",
        "#select first column in DataFrame\n",
        "df.select(df.columns[0]).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlkDOFryebGD",
        "outputId": "b7d0f71e-bb9f-4fc5-fc1d-62c4ac830c29"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|team|\n",
            "+----+\n",
            "|   A|\n",
            "|   A|\n",
            "|   A|\n",
            "|   B|\n",
            "|   B|\n",
            "|   C|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select all columns except first column in DataFrame\n",
        "df.drop(df.columns[0]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QyqJyDHer6S",
        "outputId": "10af9cf5-4951-4082-f795-b3aa2f4ec332"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+---+\n",
            "|conference|points| id|\n",
            "+----------+------+---+\n",
            "|      East|    11|  1|\n",
            "|      East|     8|  2|\n",
            "|      East|    10|  3|\n",
            "|      West|     6|  4|\n",
            "|      West|     9|  5|\n",
            "|      East|     5|  6|\n",
            "+----------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Select Range of Columns by Index\n",
        "\n",
        "#select all columns between index 0 and 2, not including 2\n",
        "df.select(df.columns[0:2]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BYYidrXevbd",
        "outputId": "427dc8dd-7d43-45b8-b8a7-5a8778610f1a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+\n",
            "|team|conference|\n",
            "+----+----------+\n",
            "|   A|      East|\n",
            "|   A|      East|\n",
            "|   A|      East|\n",
            "|   B|      West|\n",
            "|   B|      West|\n",
            "|   C|      East|\n",
            "+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7oOp5xYvezcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Select Rows Based on Column Values"
      ],
      "metadata": {
        "id": "3A_RKmMve5KO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#select rows where 'team' column is equal to 'B'\n",
        "df.where(df.team=='B').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMwiiqcye5pl",
        "outputId": "3e2730da-3ecf-4f88-8d0b-8c7b8d40b01a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+---+\n",
            "|team|conference|points| id|\n",
            "+----+----------+------+---+\n",
            "|   B|      West|     6|  4|\n",
            "|   B|      West|     9|  5|\n",
            "+----+----------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select rows where 'team' column is equal to 'A' or 'B'\n",
        "df.filter(df.team.isin('A','B')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XcdpYkye8ES",
        "outputId": "8896664b-e2e0-4725-932a-9ea7f44dfbcf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+---+\n",
            "|team|conference|points| id|\n",
            "+----+----------+------+---+\n",
            "|   A|      East|    11|  1|\n",
            "|   A|      East|     8|  2|\n",
            "|   A|      East|    10|  3|\n",
            "|   B|      West|     6|  4|\n",
            "|   B|      West|     9|  5|\n",
            "+----+----------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select rows where 'team' column is 'A' and 'points' column is greater than 9\n",
        "df.where((df.team=='A') & (df.points>9)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SobhgHkfAHg",
        "outputId": "6aefb7c8-356d-4eb4-a933-088ef5f10c09"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+---+\n",
            "|team|conference|points| id|\n",
            "+----+----------+------+---+\n",
            "|   A|      East|    11|  1|\n",
            "|   A|      East|    10|  3|\n",
            "+----+----------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select rows where 'team' column is equal to 'B'\n",
        "df.where(df.team=='B').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43qIEduZfMnz",
        "outputId": "ba13395f-45f5-4d1c-b0d5-5c8052bb7413"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+---+\n",
            "|team|conference|points| id|\n",
            "+----+----------+------+---+\n",
            "|   B|      West|     6|  4|\n",
            "|   B|      West|     9|  5|\n",
            "+----+----------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select rows where 'team' column is equal to 'A' or 'B'\n",
        "df.filter(df.team.isin('A','B')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpA2EfMsfY9Q",
        "outputId": "b25f4132-f227-4b5d-f3ea-bf793346c6f1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+---+\n",
            "|team|conference|points| id|\n",
            "+----+----------+------+---+\n",
            "|   A|      East|    11|  1|\n",
            "|   A|      East|     8|  2|\n",
            "|   A|      East|    10|  3|\n",
            "|   B|      West|     6|  4|\n",
            "|   B|      West|     9|  5|\n",
            "+----+----------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select rows where 'team' column is 'A' and 'points' column is greater than 9\n",
        "df.where((df.team=='A') & (df.points>9)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nZ0JoT3fb2b",
        "outputId": "bc52e0f6-bd6d-4485-d4c2-51477b108385"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+---+\n",
            "|team|conference|points| id|\n",
            "+----+----------+------+---+\n",
            "|   A|      East|    11|  1|\n",
            "|   A|      East|    10|  3|\n",
            "+----+----------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Keep Certain Columns in PySpark"
      ],
      "metadata": {
        "id": "zdwdxU67fk0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "#only keep columns 'col1' and 'col2'\n",
        "df.select(col('team'), col('points')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1pikCCGfgyb",
        "outputId": "840ff33f-9962-4f13-c86b-97ec4e4b64c2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|team|points|\n",
            "+----+------+\n",
            "|   A|    11|\n",
            "|   A|     8|\n",
            "|   A|    10|\n",
            "|   B|     6|\n",
            "|   B|     9|\n",
            "|   C|     5|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.drop(col('conference'), col('assists')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1jXzSCYfpsr",
        "outputId": "81b68be5-e199-461d-cde4-cd792f74a608"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+---+\n",
            "|team|points| id|\n",
            "+----+------+---+\n",
            "|   A|    11|  1|\n",
            "|   A|     8|  2|\n",
            "|   A|    10|  3|\n",
            "|   B|     6|  4|\n",
            "|   B|     9|  5|\n",
            "|   C|     5|  6|\n",
            "+----+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Select Multiple Columns in PySpark"
      ],
      "metadata": {
        "id": "r7vZ_Isshddv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#select 'team' and 'points' columns\n",
        "df.select('team', 'points').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpqLnoiVgskV",
        "outputId": "9f14d650-649a-4793-bbbf-a9cf0fba259f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|team|points|\n",
            "+----+------+\n",
            "|   A|    11|\n",
            "|   A|     8|\n",
            "|   A|    10|\n",
            "|   B|     6|\n",
            "|   B|     9|\n",
            "|   C|     5|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define list of columns to select\n",
        "select_cols = ['team', 'points']\n",
        "#select all columns in list\n",
        "df.select(*select_cols).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRwuNXQkhgkV",
        "outputId": "bea0b788-96ba-4c6e-d5f2-c8ff3461e995"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|team|points|\n",
            "+----+------+\n",
            "|   A|    11|\n",
            "|   A|     8|\n",
            "|   A|    10|\n",
            "|   B|     6|\n",
            "|   B|     9|\n",
            "|   C|     5|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select all columns between index 0 and 2 ( not including 2)\n",
        "df.select(df.columns[0:2]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89l71Gzqhjjl",
        "outputId": "7659cb2d-dfb2-43e4-9208-b20a871c357f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+\n",
            "|team|conference|\n",
            "+----+----------+\n",
            "|   A|      East|\n",
            "|   A|      East|\n",
            "|   A|      East|\n",
            "|   B|      West|\n",
            "|   B|      West|\n",
            "|   C|      East|\n",
            "+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data = [['A', 'East', 11, 4],\n",
        "['A', 'East', 8, 9],\n",
        "['A', 'East', 10, 3],\n",
        "['B', 'West', 6, 12],\n",
        "['B', 'West', 6, 4],\n",
        "['C', 'East', 5, 2]]\n",
        "#define column names\n",
        "columns = ['team', 'conference', 'points', 'assists']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ6Ch1U1hmXQ",
        "outputId": "a94af2f9-e2d3-4983-9af5-867c8c7dd8d0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      East|     8|      9|\n",
            "|   A|      East|    10|      3|\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Pc0yI48htHI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Do a Left Join in PySpark (With\n",
        "Example)"
      ],
      "metadata": {
        "id": "yQ5UBGWLiXxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntax\n",
        "# df_joined = df1.join(df2, on=['team'], how='left').show()"
      ],
      "metadata": {
        "id": "eRMNHaDSiYLy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "#define data\n",
        "data1 = [['Mavs', 11],\n",
        "         ['Hawks', 25],\n",
        "         ['Nets', 32],\n",
        "         ['Kings', 15],\n",
        "         ['Warriors', 22],\n",
        "         ['Suns', 17]]\n",
        "\n",
        "#define column names\n",
        "columns1 = ['team', 'points']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df1 = spark.createDataFrame(data1, columns1)\n",
        "\n",
        "#view dataframe\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbs8Yu9hibnD",
        "outputId": "549a3f31-26fb-4e6b-f341-6aad0b535234"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+\n",
            "|    team|points|\n",
            "+--------+------+\n",
            "|    Mavs|    11|\n",
            "|   Hawks|    25|\n",
            "|    Nets|    32|\n",
            "|   Kings|    15|\n",
            "|Warriors|    22|\n",
            "|    Suns|    17|\n",
            "+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define daata\n",
        "\n",
        "data2 = [['Mavs', 4],\n",
        "         ['Nets', 7],\n",
        "         ['Suns', 8],\n",
        "         ['Grizzlies', 12],\n",
        "         ['kings', 7]]\n",
        "\n",
        "#define column names\n",
        "columns2 = ['team', 'assists']\n",
        "\n",
        "# create dataframe using data and ccolumn names\n",
        "df2 = spark.createDataFrame(data2, columns2)\n",
        "\n",
        "#view dataframe\n",
        "df2.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtcPlh1PjD6O",
        "outputId": "0a652df0-2b9e-40a1-fbec-61ff670aebc0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|     team|assists|\n",
            "+---------+-------+\n",
            "|     Mavs|      4|\n",
            "|     Nets|      7|\n",
            "|     Suns|      8|\n",
            "|Grizzlies|     12|\n",
            "|    kings|      7|\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#perform left join using 'team' column\n",
        "df_joined = df1.join(df2, on=['team'], how='left').show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv4qnOxaj80E",
        "outputId": "33f9543c-eb4c-4218-92c0-af995cc28d06"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+-------+\n",
            "|    team|points|assists|\n",
            "+--------+------+-------+\n",
            "|    Mavs|    11|      4|\n",
            "|    Nets|    32|      7|\n",
            "|   Hawks|    25|   NULL|\n",
            "|   Kings|    15|   NULL|\n",
            "|Warriors|    22|   NULL|\n",
            "|    Suns|    17|      8|\n",
            "+--------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qARUT_ahkBs3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark: How to Do a Left Join on Multiple\n",
        "Columns"
      ],
      "metadata": {
        "id": "WZP0DcwnkG8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# syntax\n",
        "#df_joined = df1.join(df2, on=[df1.col1==df2.col1, df1.col2==df2.col2], how='left')"
      ],
      "metadata": {
        "id": "4SAPe_HxkHYR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Left Join on Multiple Columns in PySpark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data1 = [['A', 'G', 18],\n",
        "         ['A', 'F', 22],\n",
        "         ['B', 'F', 19],\n",
        "         ['B', 'G', 14]]\n",
        "\n",
        "\n",
        "#define column names\n",
        "columns1 = ['team', 'pos', 'points']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df1 = spark.createDataFrame(data1, columns1)\n",
        "\n",
        "#view dataframe\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS-t_T4akN7R",
        "outputId": "4a29536a-5da0-4b3f-c0a9-ecd3278b9e11"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+------+\n",
            "|team|pos|points|\n",
            "+----+---+------+\n",
            "|   A|  G|    18|\n",
            "|   A|  F|    22|\n",
            "|   B|  F|    19|\n",
            "|   B|  G|    14|\n",
            "+----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define data\n",
        "data2 = [['A', 'G', 4],\n",
        "         ['A', 'F', 9],\n",
        "         ['B', 'F', 8],\n",
        "         ['C', 'G', 6],\n",
        "         ['C', 'F', 5]]\n",
        "#define column names\n",
        "columns2 = ['team_name', 'position', 'assists']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df2 = spark.createDataFrame(data2, columns2)\n",
        "\n",
        "#view dataframe\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnVBMM2sknmv",
        "outputId": "2a3d5b3a-ecbc-4b64-97fb-bda6f8040368"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+-------+\n",
            "|team_name|position|assists|\n",
            "+---------+--------+-------+\n",
            "|        A|       G|      4|\n",
            "|        A|       F|      9|\n",
            "|        B|       F|      8|\n",
            "|        C|       G|      6|\n",
            "|        C|       F|      5|\n",
            "+---------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#perform left join\n",
        "df_joined = df1.join(df2, on=[df1.team==df2.team_name,df1.pos==df2.position], how='left')\n",
        "\n",
        "#view resulting DataFrame\n",
        "df_joined.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZi0vvLNk0hh",
        "outputId": "74bf2ccd-4078-47fd-fcca-d7ced36df242"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+------+---------+--------+-------+\n",
            "|team|pos|points|team_name|position|assists|\n",
            "+----+---+------+---------+--------+-------+\n",
            "|   A|  G|    18|        A|       G|      4|\n",
            "|   A|  F|    22|        A|       F|      9|\n",
            "|   B|  G|    14|     NULL|    NULL|   NULL|\n",
            "|   B|  F|    19|        B|       F|      8|\n",
            "+----+---+------+---------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#drop 'team_name' and 'position' columns from joined DataFrame\n",
        "df_joined.drop('team_name', 'position').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFeuqil8lKss",
        "outputId": "492d9a77-1c6d-4959-bbe9-86affb54e21b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+------+-------+\n",
            "|team|pos|points|assists|\n",
            "+----+---+------+-------+\n",
            "|   A|  G|    18|      4|\n",
            "|   A|  F|    22|      9|\n",
            "|   B|  G|    14|   NULL|\n",
            "|   B|  F|    19|      8|\n",
            "+----+---+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ahL18VGlTu5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Do a Right Join in PySpark"
      ],
      "metadata": {
        "id": "5-xzkYrllWuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  SYNTAX\n",
        "# df_joined = df1.join(df2, on=['team'], how='right ').show()"
      ],
      "metadata": {
        "id": "GjshfyAUlXDV"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data1 = [['Mavs', 11],\n",
        "         ['Hawks', 25],\n",
        "         ['Nets', 32],\n",
        "         ['kINGS', 15],\n",
        "         ['Warriors', 22],\n",
        "         ['Suns', 17]]\n",
        "\n",
        "#define column names\n",
        "columns1 = ['team', 'points']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df1 = spark.createDataFrame(data1, columns1)\n",
        "\n",
        "#view dataframe\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTvnMnKflk6-",
        "outputId": "e4266f52-9686-4573-bf3a-a77146387d9c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+\n",
            "|    team|points|\n",
            "+--------+------+\n",
            "|    Mavs|    11|\n",
            "|   Hawks|    25|\n",
            "|    Nets|    32|\n",
            "|   kINGS|    15|\n",
            "|Warriors|    22|\n",
            "|    Suns|    17|\n",
            "+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define data\n",
        "data2 = [['Mavs', 4],\n",
        "        ['Nets', 7],\n",
        "        ['Suns', 8],\n",
        "        ['Grizzlies', 12],\n",
        "        ['Kings', 7]]\n",
        "#define column names\n",
        "columns2 = ['team', 'assists']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df2 = spark.createDataFrame(data2, columns2)\n",
        "\n",
        "#view dataframe\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZZZIcq1lk4j",
        "outputId": "4781ede6-3841-41a0-f5a1-36a06396ca1b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|     team|assists|\n",
            "+---------+-------+\n",
            "|     Mavs|      4|\n",
            "|     Nets|      7|\n",
            "|     Suns|      8|\n",
            "|Grizzlies|     12|\n",
            "|    Kings|      7|\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_joined = df1.join(df2, on=['team'], how='right').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teWtM7u4lbcX",
        "outputId": "8c8f8020-007b-41bc-afd6-09f970909475"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+-------+\n",
            "|     team|points|assists|\n",
            "+---------+------+-------+\n",
            "|     Mavs|    11|      4|\n",
            "|     Nets|    32|      7|\n",
            "|    Kings|  NULL|      7|\n",
            "|Grizzlies|  NULL|     12|\n",
            "|     Suns|    17|      8|\n",
            "+---------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yrCOWkqilgxM"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Perform an Anti-Join in PySpark"
      ],
      "metadata": {
        "id": "Xbiidijymre-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''An anti-join allows you to return all rows in one DataFrame that do not have matching values in\n",
        "another DataFrame.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fLOftYkZmr4d",
        "outputId": "6c6d867a-b1ef-4e54-9f30-6717e72c3735"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'An anti-join allows you to return all rows in one DataFrame that do not have matching values in\\nanother DataFrame.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SYNTAX\n",
        "# df_anti_join = df1.join(df2, on=['team'], how='left_anti')"
      ],
      "metadata": {
        "id": "r19e2lYkm1HL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "\n",
        "#define data\n",
        "data1 = [['A', 18],\n",
        "         ['B', 22],\n",
        "         ['C', 19],\n",
        "         ['D', 14],\n",
        "         ['E', 30]]\n",
        "\n",
        "\n",
        "#define column names\n",
        "columns1 = ['team', 'points']\n",
        "\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df1 = spark.createDataFrame(data1, columns1)\n",
        "\n",
        "\n",
        "#view dataframe\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n12-lh5Qm7O_",
        "outputId": "65f20873-87b6-49b7-91ce-941d528c35d1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|team|points|\n",
            "+----+------+\n",
            "|   A|    18|\n",
            "|   B|    22|\n",
            "|   C|    19|\n",
            "|   D|    14|\n",
            "|   E|    30|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define data\n",
        "data2 = [['A', 18],\n",
        "         ['B', 22],\n",
        "         ['C', 19],\n",
        "         ['F', 22],\n",
        "         ['G', 29]]\n",
        "\n",
        "#define column names\n",
        "columns2 = ['team', 'points']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df2 = spark.createDataFrame(data2, columns2)\n",
        "\n",
        "#view dataframe\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLvDUGvonF_K",
        "outputId": "0cacb491-8237-4bf6-a906-0b6e9ffdd35c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|team|points|\n",
            "+----+------+\n",
            "|   A|    18|\n",
            "|   B|    22|\n",
            "|   C|    19|\n",
            "|   F|    22|\n",
            "|   G|    29|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#perform anti-join\n",
        "df_anti_join = df1.join(df2, on=['team'], how='left_anti')\n",
        "\n",
        "#view resulting DataFrame\n",
        "df_anti_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9nP09uanSX2",
        "outputId": "a2c7aa09-af39-413b-ae66-06c1e6244a73"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|team|points|\n",
            "+----+------+\n",
            "|   E|    30|\n",
            "|   D|    14|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Do an Outer Join in PySpark (With\n",
        "Example)"
      ],
      "metadata": {
        "id": "goe3hPDGnhIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SYNTAX\n",
        "#df_joined = df1.join(df2, on=['team'], how='full').show()"
      ],
      "metadata": {
        "id": "XtgF90lVnaQR"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data1 = [['Mavs', 11],\n",
        "         ['Hawks', 25],\n",
        "         ['Nets', 32],\n",
        "         ['Kings', 15],\n",
        "         ['Warriors', 22],\n",
        "         ['Suns', 17]]\n",
        "#define column names\n",
        "columns1 = ['team', 'points']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df1 = spark.createDataFrame(data1, columns1)\n",
        "\n",
        "#view dataframe\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1Bra1AHnrzZ",
        "outputId": "fb3b6dff-0f7b-4905-95e2-9e9f6fd4d0bf"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+\n",
            "|    team|points|\n",
            "+--------+------+\n",
            "|    Mavs|    11|\n",
            "|   Hawks|    25|\n",
            "|    Nets|    32|\n",
            "|   Kings|    15|\n",
            "|Warriors|    22|\n",
            "|    Suns|    17|\n",
            "+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define data\n",
        "data2 = [['Mavs', 4],\n",
        "         ['Nets', 7],\n",
        "         ['Suns', 8],\n",
        "         ['Grizzlies', 12],\n",
        "         ['Kings', 7]]\n",
        "#define column names\n",
        "columns2 = ['team', 'assists']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df2 = spark.createDataFrame(data2, columns2)\n",
        "\n",
        "\n",
        "#view dataframe\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4_lrq8soIOG",
        "outputId": "0c00da84-9294-4173-d964-03c1efb616b2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|     team|assists|\n",
            "+---------+-------+\n",
            "|     Mavs|      4|\n",
            "|     Nets|      7|\n",
            "|     Suns|      8|\n",
            "|Grizzlies|     12|\n",
            "|    Kings|      7|\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#perform outer join using 'team' column\n",
        "df_joined = df1.join(df2, on=['team'], how='full').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ98Q28coR8A",
        "outputId": "9220084d-faab-49d1-f3fa-90b61ee07641"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+-------+\n",
            "|     team|points|assists|\n",
            "+---------+------+-------+\n",
            "|Grizzlies|  NULL|     12|\n",
            "|    Hawks|    25|   NULL|\n",
            "|    Kings|    15|      7|\n",
            "|     Mavs|    11|      4|\n",
            "|     Nets|    32|      7|\n",
            "|     Suns|    17|      8|\n",
            "| Warriors|    22|   NULL|\n",
            "+---------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSCbD5-qppIL"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Do an Inner Join in PySpark (With\n",
        "Example)"
      ],
      "metadata": {
        "id": "Gy9z9NkMprPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#You can use the following basic syntax to perform an inner join in PySpark:\n",
        "# df_joined = df1.join(df2, on=['team'], how='inner').show()"
      ],
      "metadata": {
        "id": "Tpeir-CZprjB"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data1 = [['Mavs', 11],\n",
        "         ['Hawks', 25],\n",
        "         ['Nets', 32],\n",
        "         ['Kings', 15],\n",
        "         ['Warriors', 22],\n",
        "         ['Suns', 17]]\n",
        "\n",
        "#define column names\n",
        "columns1 = ['team', 'points']\n",
        "\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df1 = spark.createDataFrame(data1, columns1)\n",
        "\n",
        "#view dataframe\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L53zrkh6pvG8",
        "outputId": "193478e1-f2c8-4271-eeb2-0e5f85bd2b26"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+\n",
            "|    team|points|\n",
            "+--------+------+\n",
            "|    Mavs|    11|\n",
            "|   Hawks|    25|\n",
            "|    Nets|    32|\n",
            "|   Kings|    15|\n",
            "|Warriors|    22|\n",
            "|    Suns|    17|\n",
            "+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define data\n",
        "data2 = [['Mavs', 4],\n",
        "        ['Nets', 7],\n",
        "        ['Suns', 8],\n",
        "        ['Grizzlies', 12],\n",
        "        ['Kings', 7]]\n",
        "#define column names\n",
        "columns2 = ['team', 'assists']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df2 = spark.createDataFrame(data2, columns2)\n",
        "\n",
        "#view dataframe\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-SPtK1dp42S",
        "outputId": "76d5501a-ee7b-4437-a0b6-8520a9e5d393"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|     team|assists|\n",
            "+---------+-------+\n",
            "|     Mavs|      4|\n",
            "|     Nets|      7|\n",
            "|     Suns|      8|\n",
            "|Grizzlies|     12|\n",
            "|    Kings|      7|\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#perform inner join using 'team' column\n",
        "df_joined = df1.join(df2, on=['team'], how='inner').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCSN8be4p8jQ",
        "outputId": "875c77e9-a236-402b-be08-0ffc9a1ddab4"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-------+\n",
            "| team|points|assists|\n",
            "+-----+------+-------+\n",
            "|Kings|    15|      7|\n",
            "| Mavs|    11|      4|\n",
            "| Nets|    32|      7|\n",
            "| Suns|    17|      8|\n",
            "+-----+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VOXh0kjbqD3C"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark: How to Join on Different Column\n",
        "Names"
      ],
      "metadata": {
        "id": "6TOK3WokqFia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SYNTAX\n",
        "#df3 = df1.withColumn('id',col('team_id')).join(df2.withColumn('id', col('team_name')),on='id')"
      ],
      "metadata": {
        "id": "FIFLxsUOqF5E"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How to Join on Different Column Names in PySpark"
      ],
      "metadata": {
        "id": "suEFTotbqMmc"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [['Mavs', 18],\n",
        "['Nets', 33],\n",
        "['Lakers', 12],\n",
        "['Kings', 15],\n",
        "['Hawks', 19],\n",
        "['Wizards', 24],\n",
        "['Magic', 28]]\n",
        "#define column names\n",
        "columns = ['team_ID', 'points']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df1 = spark.createDataFrame(data, columns)\n",
        "\n",
        "#view dataframe\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yafk3qgZqWi7",
        "outputId": "fda7bdda-972a-4d5c-9221-94dbf88b9c8e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|team_ID|points|\n",
            "+-------+------+\n",
            "|   Mavs|    18|\n",
            "|   Nets|    33|\n",
            "| Lakers|    12|\n",
            "|  Kings|    15|\n",
            "|  Hawks|    19|\n",
            "|Wizards|    24|\n",
            "|  Magic|    28|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define data\n",
        "data = [['Hawks', 4],\n",
        "['Wizards', 5],\n",
        "['Raptors', 5],\n",
        "['Kings', 12],\n",
        "['Mavs', 7],\n",
        "['Nets', 11],\n",
        "['Magic', 3]]\n",
        "\n",
        "#define column names\n",
        "columns = ['team_name', 'assists']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df2 = spark.createDataFrame(data, columns)\n",
        "\n",
        "#view dataframe\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx1alJo9qYLn",
        "outputId": "ab5e6800-0487-4826-cc87-16c2867eae53"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|team_name|assists|\n",
            "+---------+-------+\n",
            "|    Hawks|      4|\n",
            "|  Wizards|      5|\n",
            "|  Raptors|      5|\n",
            "|    Kings|     12|\n",
            "|     Mavs|      7|\n",
            "|     Nets|     11|\n",
            "|    Magic|      3|\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#join df1 and df2 on different column names\n",
        "df3 = df1.withColumn('id',\n",
        "col('team_id')).join(df2.withColumn('id', col('team_name')),\n",
        "on='id')\n",
        "\n",
        "#view resulting DataFrame\n",
        "df3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1mWT-eFqbb8",
        "outputId": "aefd413c-b831-4836-bf97-f63472372151"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+------+---------+-------+\n",
            "|     id|team_ID|points|team_name|assists|\n",
            "+-------+-------+------+---------+-------+\n",
            "|  Hawks|  Hawks|    19|    Hawks|      4|\n",
            "|  Kings|  Kings|    15|    Kings|     12|\n",
            "|  Magic|  Magic|    28|    Magic|      3|\n",
            "|   Mavs|   Mavs|    18|     Mavs|      7|\n",
            "|   Nets|   Nets|    33|     Nets|     11|\n",
            "|Wizards|Wizards|    24|  Wizards|      5|\n",
            "+-------+-------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#join df1 and df2 on different column names\n",
        "df3 = df1.withColumn('id',col('team_id')).join(df2.withColumn('id', col('team_name')),on='id').select('id', 'points', 'assists')"
      ],
      "metadata": {
        "id": "zpxKpbarqgxQ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#view resulting DataFrame\n",
        "df3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsOCEBBEqlZz",
        "outputId": "60f73fa5-117e-46a6-ce03-24656aa96742"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-------+\n",
            "|     id|points|assists|\n",
            "+-------+------+-------+\n",
            "|  Hawks|    19|      4|\n",
            "|  Kings|    15|     12|\n",
            "|  Magic|    28|      3|\n",
            "|   Mavs|    18|      7|\n",
            "|   Nets|    33|     11|\n",
            "|Wizards|    24|      5|\n",
            "+-------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Print One Column of a PySpark\n",
        "DataFrame"
      ],
      "metadata": {
        "id": "6AwgOpKSq4tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print 'conference' column (with column name)\n",
        "df.select('conference').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txj3cw6Tqp_Y",
        "outputId": "184e5904-5da0-4cec-9ada-038ec1653974"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|conference|\n",
            "+----------+\n",
            "|      East|\n",
            "|      East|\n",
            "|      East|\n",
            "|      West|\n",
            "|      West|\n",
            "|      East|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print values only from 'conference' column\n",
        "df.select('conference').rdd.flatMap(list).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s07Lw0oTrAJH",
        "outputId": "76d4258a-1463-48f3-872a-544c917caba3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['East', 'East', 'East', 'West', 'West', 'East']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3l705mLFrDut"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Check if Column Contains\n",
        "String"
      ],
      "metadata": {
        "id": "lvep2hM-rR0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check if 'conference' column contains exact string 'Eas' in any row\n",
        "df.where(df.conference=='East').count()>0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evaQVHcvrSI-",
        "outputId": "5ca871c5-2337-4cb3-ed00-bca67fa70c23"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check if 'conference' column contains partial string 'Eas' in any row\n",
        "df.filter(df.conference.contains('East')).count()>0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgrac-jyrW79",
        "outputId": "a39f991d-8a28-4a21-bd20-4de56aab0976"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#count occurrences of partial string 'Eas' in 'conference' column\n",
        "df.filter(df.conference.contains('Eas')).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec-9vNLOrZ5V",
        "outputId": "86239d03-fbbf-4b43-caec-3eecd571cb3a"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define data\n",
        "data = [['A', 'East', 11],\n",
        "['A', 'East', 8],\n",
        "['A', 'East', 10],\n",
        "['B', 'West', 6],\n",
        "['B', 'West', 6],\n",
        "['C', 'East', 5]]\n",
        "\n",
        "#define column names\n",
        "columns = ['team', 'conference', 'points']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLjHhAy2rhvS",
        "outputId": "d113cfce-4e9b-414c-c582-854c243862c0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+\n",
            "|team|conference|points|\n",
            "+----+----------+------+\n",
            "|   A|      East|    11|\n",
            "|   A|      East|     8|\n",
            "|   A|      East|    10|\n",
            "|   B|      West|     6|\n",
            "|   B|      West|     6|\n",
            "|   C|      East|     5|\n",
            "+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check if column name 'points' exists in the DataFrame\n",
        "'points' in df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go5riTS9rly0",
        "outputId": "267b9c4c-0742-4d8a-8df1-9193687a9893"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l7PdGH6ar0AP"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Check if DataFrame is\n",
        "Empty"
      ],
      "metadata": {
        "id": "Guyp_vZnsJya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
        "\n",
        "#create empty RDD\n",
        "empty_rdd=spark.sparkContext.emptyRDD()\n",
        "\n",
        "\n",
        "#specify colum names and types\n",
        "my_columns=[StructField('team', StringType(),True),\n",
        "StructField('position', StringType(),True),\n",
        "StructField('points', FloatType(),True)]\n",
        "\n",
        "#create DataFrame with specific column names\n",
        "df=spark.createDataFrame([], schema=StructType(my_columns))\n",
        "\n",
        "#view DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AHxNdM4sKG5",
        "outputId": "240626fe-6644-4e16-fa21-77a626bc32dd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+------+\n",
            "|team|position|points|\n",
            "+----+--------+------+\n",
            "+----+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check if DataFrame is empty\n",
        "print(df.count() == 0)\n",
        "True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpPmW2rUsR6t",
        "outputId": "ddc3557d-6605-4b12-b0dd-d691912769b4"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define data\n",
        "data = [['Mavs', 18],\n",
        "['Nets', 33],\n",
        "['Lakers', 12],\n",
        "['Mavs', 15],\n",
        "['Cavs', 19],\n",
        "['Wizards', 24],]\n",
        "#define column names\n",
        "columns = ['team', 'points']\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb5iQ3pasU82",
        "outputId": "17f42cd9-569e-4d8c-f6fd-f2ac73c79459"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|   team|points|\n",
            "+-------+------+\n",
            "|   Mavs|    18|\n",
            "|   Nets|    33|\n",
            "| Lakers|    12|\n",
            "|   Mavs|    15|\n",
            "|   Cavs|    19|\n",
            "|Wizards|    24|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check if DataFrame is empty\n",
        "print(df.count() == 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHbPAaJbsYvW",
        "outputId": "fde55a94-62d2-4d42-d007-c4276114bd58"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JYAqHMT7scTl"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Check Data Type of\n",
        "Columns in DataFrame"
      ],
      "metadata": {
        "id": "329Z4lStsnAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUW8RR6aswED",
        "outputId": "213b9eaa-bb7d-4ae0-b6e7-ce1ecff484f9"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|   team|points|\n",
            "+-------+------+\n",
            "|   Mavs|    18|\n",
            "|   Nets|    33|\n",
            "| Lakers|    12|\n",
            "|   Mavs|    15|\n",
            "|   Cavs|    19|\n",
            "|Wizards|    24|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#return data type of 'conference' column\n",
        "dict(df.dtypes)['team']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ChnRCGKnsnZF",
        "outputId": "96daa13c-7d2d-4d8e-ca93-a27ef100ecea"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'string'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#return data type of all columns\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uRIFmugsqOw",
        "outputId": "7e41db3d-c843-43a1-c648-545b8c4be451"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('team', 'string'), ('points', 'bigint')]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define data\n",
        "data = [['A', 'East', 11, 4],\n",
        "        ['A', None, 8, 9],\n",
        "        ['A', 'East', 10, 3],\n",
        "        ['B', 'West', None, 12],\n",
        "        ['B', 'West', None, 4],\n",
        "        ['C', 'East', 5, 2]]\n",
        "\n",
        "\n",
        "#define column names\n",
        "columns = ['team', 'conference', 'points', 'assists']\n",
        "\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYvqmXP0s0TQ",
        "outputId": "5e722753-ee51-4814-a486-572dcf801c85"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      NULL|     8|      9|\n",
            "|   A|      East|    10|      3|\n",
            "|   B|      West|  NULL|     12|\n",
            "|   B|      West|  NULL|      4|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict(df.dtypes)['conference']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "t3dpSENis_WA",
        "outputId": "226f4a40-3cd8-4eca-cf1b-77c1c330020f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'string'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGtO_UyVtJzq",
        "outputId": "461234cd-830c-44af-f4b1-c8527c3839e5"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('team', 'string'),\n",
              " ('conference', 'string'),\n",
              " ('points', 'bigint'),\n",
              " ('assists', 'bigint')]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZnnWweR8tPvw"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark: How to Drop Multiple Columns\n",
        "from DataFrame"
      ],
      "metadata": {
        "id": "OFddYbCztSRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#drop 'team' and 'points' columns\n",
        "df.drop('team', 'points').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPJL9QYRtSnK",
        "outputId": "368af578-1cca-4104-ab14-dffa4fcc8690"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+\n",
            "|conference|assists|\n",
            "+----------+-------+\n",
            "|      East|      4|\n",
            "|      NULL|      9|\n",
            "|      East|      3|\n",
            "|      West|     12|\n",
            "|      West|      4|\n",
            "|      East|      2|\n",
            "+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define list of columns to drop\n",
        "drop_cols = ['team', 'points']\n",
        "\n",
        "#drop all columns in list\n",
        "df.select(*drop_cols).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJS96klytZTM",
        "outputId": "d8f6d023-2258-4cdf-98e7-a02c7eb31709"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|team|points|\n",
            "+----+------+\n",
            "|   A|    11|\n",
            "|   A|     8|\n",
            "|   A|    10|\n",
            "|   B|  NULL|\n",
            "|   B|  NULL|\n",
            "|   C|     5|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define data\n",
        "data = [['A', 'East', 11, 4],\n",
        "      ['A', 'East', 8, 9],\n",
        "        ['A', 'East', 10, 3],\n",
        "      ['B', 'West', 6, 12],\n",
        "      ['B', 'West', 6, 4],\n",
        "      ['C', 'East', 5, 2]]\n",
        "#define column names\n",
        "columns = ['team', 'conference', 'points', 'assists']\n",
        "\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDrWyV5gtfTj",
        "outputId": "7078ea1b-d73d-4e68-af20-8df0c7586c4f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      East|     8|      9|\n",
            "|   A|      East|    10|      3|\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Drop Multiple Columns by Name\n",
        "\n",
        "#drop 'team' and 'points' columns\n",
        "df.drop('team', 'points').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JITND3FtmCt",
        "outputId": "36546dd8-dd94-4db0-cb05-9c81551f2713"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+\n",
            "|conference|assists|\n",
            "+----------+-------+\n",
            "|      East|      4|\n",
            "|      East|      9|\n",
            "|      East|      3|\n",
            "|      West|     12|\n",
            "|      West|      4|\n",
            "|      East|      2|\n",
            "+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vg4ia42UtyBu"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Drop Duplicate Rows from\n",
        "DataFrame"
      ],
      "metadata": {
        "id": "xdJym1lct68Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define data\n",
        "data = [['A', 'Guard', 11],\n",
        "['A', 'Guard', 8],\n",
        "['A', 'Forward', 22],\n",
        "['A', 'Forward', 22],\n",
        "['B', 'Guard', 14],\n",
        "['B', 'Guard', 14],\n",
        "['B', 'Forward', 13],\n",
        "['B', 'Forward', 7]]\n",
        "\n",
        "\n",
        "#define column names\n",
        "columns = ['team', 'position', 'points']\n",
        "\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmRt5hlQt7Ud",
        "outputId": "2de36010-d630-4e54-d863-47a1200a6b5b"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+------+\n",
            "|team|position|points|\n",
            "+----+--------+------+\n",
            "|   A|   Guard|    11|\n",
            "|   A|   Guard|     8|\n",
            "|   A| Forward|    22|\n",
            "|   A| Forward|    22|\n",
            "|   B|   Guard|    14|\n",
            "|   B|   Guard|    14|\n",
            "|   B| Forward|    13|\n",
            "|   B| Forward|     7|\n",
            "+----+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#drop rows that have duplicate values across all columns\n",
        "df_new = df.dropDuplicates()\n",
        "#view DataFrame without duplicates\n",
        "df_new.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "063DWZq7uAv5",
        "outputId": "7d74f73d-282d-41e8-b66c-1ff211247085"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+------+\n",
            "|team|position|points|\n",
            "+----+--------+------+\n",
            "|   A|   Guard|     8|\n",
            "|   A|   Guard|    11|\n",
            "|   A| Forward|    22|\n",
            "|   B| Forward|     7|\n",
            "|   B|   Guard|    14|\n",
            "|   B| Forward|    13|\n",
            "+----+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#drop rows that have duplicate values across 'team' and 'position'\n",
        "columns\n",
        "df_new = df.dropDuplicates(['team', 'position'])\n",
        "#view DataFrame without duplicates\n",
        "df_new.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3a3yU9CuEaR",
        "outputId": "89c3baed-7d54-491c-d0dc-bf008a337ea7"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+------+\n",
            "|team|position|points|\n",
            "+----+--------+------+\n",
            "|   A| Forward|    22|\n",
            "|   A|   Guard|    11|\n",
            "|   B| Forward|    13|\n",
            "|   B|   Guard|    14|\n",
            "+----+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#drop rows that have duplicate values in 'team' column\n",
        "df_new = df.dropDuplicates(['team'])\n",
        "#view DataFrame without duplicates\n",
        "df_new.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQac3WsyuNRt",
        "outputId": "9f514bbf-5399-4d77-e63c-d2e08f8aae1a"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+------+\n",
            "|team|position|points|\n",
            "+----+--------+------+\n",
            "|   A|   Guard|    11|\n",
            "|   B|   Guard|    14|\n",
            "+----+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aY28OxRruQVS"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Select Distinct Rows in PySpark\n",
        "(With Examples)"
      ],
      "metadata": {
        "id": "_JWw_qEEwgUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data = [['A', 'Guard', 11],\n",
        "['A', 'Guard', 8],\n",
        "['A', 'Forward', 22],\n",
        "['A', 'Forward', 22],\n",
        "['B', 'Guard', 14],\n",
        "['B', 'Guard', 14],\n",
        "['B', 'Forward', 13],\n",
        "['B', 'Forward', 7]]\n",
        "#define column names\n",
        "columns = ['team', 'position', 'points']\n",
        "#create DataFrame using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "#view DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0w1FUHBwgr0",
        "outputId": "5c9e62d4-7a86-4654-8abe-357045edf733"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+------+\n",
            "|team|position|points|\n",
            "+----+--------+------+\n",
            "|   A|   Guard|    11|\n",
            "|   A|   Guard|     8|\n",
            "|   A| Forward|    22|\n",
            "|   A| Forward|    22|\n",
            "|   B|   Guard|    14|\n",
            "|   B|   Guard|    14|\n",
            "|   B| Forward|    13|\n",
            "|   B| Forward|     7|\n",
            "+----+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#display distinct rows only\n",
        "df.distinct().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP_PSK9GwkM8",
        "outputId": "a090afc6-f7f0-4291-8011-e911b808c060"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+------+\n",
            "|team|position|points|\n",
            "+----+--------+------+\n",
            "|   A|   Guard|     8|\n",
            "|   A|   Guard|    11|\n",
            "|   A| Forward|    22|\n",
            "|   B| Forward|     7|\n",
            "|   B|   Guard|    14|\n",
            "|   B| Forward|    13|\n",
            "+----+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#display distinct values from 'team' column only\n",
        "df.select('team').distinct().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtaEQcFowmRQ",
        "outputId": "cadc1941-0a29-4697-828b-ff1a778a35e0"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|team|\n",
            "+----+\n",
            "|   A|\n",
            "|   B|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#count number of distinct rows\n",
        "df.distinct().count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWGWdd3Mwrd8",
        "outputId": "db7e8bbd-f757-41a2-9b6f-3bcb4278ecaf"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_gPjg5yfwu3d"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark: How to Select Columns with Alias"
      ],
      "metadata": {
        "id": "YZOgUr7nw1RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data = [['A', 'East', 11, 4],\n",
        "['A', 'East', 8, 9],\n",
        "['A', 'East', 10, 3],\n",
        "['B', 'West', 6, 12],\n",
        "['B', 'West', 6, 4],\n",
        "['C', 'East', 5, 2]]\n",
        "#define column names\n",
        "columns = ['team', 'conference', 'points', 'assists']\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vLpTA5Vw1l9",
        "outputId": "e4404e3c-071a-4207-d730-07516b38560c"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      East|     8|      9|\n",
            "|   A|      East|    10|      3|\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select 'team' column and display using aliased name of 'team_name'\n",
        "df.select(df.team.alias('team_name')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xqIC1pDw5L-",
        "outputId": "8f5dddd6-8a68-4e9b-c615-19fd966ad1d9"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|team_name|\n",
            "+---------+\n",
            "|        A|\n",
            "|        A|\n",
            "|        A|\n",
            "|        B|\n",
            "|        B|\n",
            "|        C|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cikSCnWzxCNE"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Select Top N Rows in PySpark\n",
        "DataFrame (With Examples)"
      ],
      "metadata": {
        "id": "8JZNYELmxMSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data = [['A', 'East', 11],\n",
        "['A', 'East', 8],\n",
        "['A', 'East', 10],\n",
        "['B', 'West', 6],\n",
        "['B', 'West', 6],\n",
        "['C', 'East', 5]]\n",
        "#define column names\n",
        "columns = ['team', 'conference', 'points']\n",
        "#create DataFrame using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "#view DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC7JXmhXxMnO",
        "outputId": "ac93e00e-d644-4772-9f38-2371898fbcba"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+\n",
            "|team|conference|points|\n",
            "+----+----------+------+\n",
            "|   A|      East|    11|\n",
            "|   A|      East|     8|\n",
            "|   A|      East|    10|\n",
            "|   B|      West|     6|\n",
            "|   B|      West|     6|\n",
            "|   C|      East|     5|\n",
            "+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select top 3 rows from DataFrame\n",
        "df.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t7tKicKxPBn",
        "outputId": "3a3d2bc4-5a94-4426-fd72-38ddb3577556"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(team='A', conference='East', points=11),\n",
              " Row(team='A', conference='East', points=8),\n",
              " Row(team='A', conference='East', points=10)]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select top 3 rows from DataFrame\n",
        "df.limit(3).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC8eYGj8xTUd",
        "outputId": "d8063f0b-e574-42da-a7c8-7467c6052b70"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+\n",
            "|team|conference|points|\n",
            "+----+----------+------+\n",
            "|   A|      East|    11|\n",
            "|   A|      East|     8|\n",
            "|   A|      East|    10|\n",
            "+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select top 3 rows from DataFrame only for 'team' and 'points columns\n",
        "df.select('team', 'points').limit(3).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqrEJY0zxVGJ",
        "outputId": "e1020d6b-3029-4f46-8269-81721a67f8d8"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|team|points|\n",
            "+----+------+\n",
            "|   A|    11|\n",
            "|   A|     8|\n",
            "|   A|    10|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark: Select All Columns Except Specific\n",
        "Ones"
      ],
      "metadata": {
        "id": "_LE6clQCxdzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data = [['A', 'East', 11, 4],\n",
        "['A', 'East', 8, 9],\n",
        "['A', 'East', 10, 3],\n",
        "['B', 'West', 6, 12],['B', 'West', 6, 4],\n",
        "['C', 'East', 5, 2]]\n",
        "#define column names\n",
        "columns = ['team', 'conference', 'points', 'assists']\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpKsSU6jxXt9",
        "outputId": "508eebe7-cdd0-4e15-9696-95093be5bca9"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      East|     8|      9|\n",
            "|   A|      East|    10|      3|\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select all columns except 'conference' column\n",
        "df.drop('conference').show()"
      ],
      "metadata": {
        "id": "1tiMpY-exilM",
        "outputId": "25c045f4-7c12-4fa0-8ca4-7d5c6df6aeba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+-------+\n",
            "|team|points|assists|\n",
            "+----+------+-------+\n",
            "|   A|    11|      4|\n",
            "|   A|     8|      9|\n",
            "|   A|    10|      3|\n",
            "|   B|     6|     12|\n",
            "|   B|     6|      4|\n",
            "|   C|     5|      2|\n",
            "+----+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select all columns except 'conference' and 'assists' column\n",
        "df.drop('conference', 'assists').show()"
      ],
      "metadata": {
        "id": "sByMDDQwxk8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2c1791e-152b-456b-c8f4-b41607bf9403"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|team|points|\n",
            "+----+------+\n",
            "|   A|    11|\n",
            "|   A|     8|\n",
            "|   A|    10|\n",
            "|   B|     6|\n",
            "|   B|     6|\n",
            "|   C|     5|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How to Use a Case Statement in PySpark (With Example)"
      ],
      "metadata": {
        "id": "egdbpeWRPxdq"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data = [['A', 6],\n",
        "['B', 8],\n",
        "['C', 9],\n",
        "['D', 9],\n",
        "['E', 12],\n",
        "['F', 14],\n",
        "['G', 15],\n",
        "['H', 17],\n",
        "['I', 19],\n",
        "['J', 22]]\n",
        "#define column names\n",
        "columns = ['player', 'points']\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RST-D18WP3mS",
        "outputId": "54a74885-d599-405d-d01d-9f05159830d1"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+\n",
            "|player|points|\n",
            "+------+------+\n",
            "|     A|     6|\n",
            "|     B|     8|\n",
            "|     C|     9|\n",
            "|     D|     9|\n",
            "|     E|    12|\n",
            "|     F|    14|\n",
            "|     G|    15|\n",
            "|     H|    17|\n",
            "|     I|    19|\n",
            "|     J|    22|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "df.withColumn('class',when(df.points<9, 'Bad').when(df.points<12,'OK').when(df.points<15, 'Good').otherwise('Great')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQjLE_sVP6Ly",
        "outputId": "3432a96f-6ee2-4403-91c3-0349e369bc59"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----+\n",
            "|player|points|class|\n",
            "+------+------+-----+\n",
            "|     A|     6|  Bad|\n",
            "|     B|     8|  Bad|\n",
            "|     C|     9|   OK|\n",
            "|     D|     9|   OK|\n",
            "|     E|    12| Good|\n",
            "|     F|    14| Good|\n",
            "|     G|    15|Great|\n",
            "|     H|    17|Great|\n",
            "|     I|    19|Great|\n",
            "|     J|    22|Great|\n",
            "+------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark: How to Convert Column from Date\n",
        "to String"
      ],
      "metadata": {
        "id": "Egk35xRRQHa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "import datetime\n",
        "#define data\n",
        "data = [[datetime.date(2023, 10, 30), 136],\n",
        "[datetime.date(2023, 11, 14), 223],\n",
        "[datetime.date(2023, 11, 22), 450],\n",
        "[datetime.date(2023, 11, 25), 290],\n",
        "[datetime.date(2023, 12, 19), 189]]\n",
        "#define column names\n",
        "columns = ['date', 'sales']\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "#view dataframe with full column content\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHhh0stFQBri",
        "outputId": "77584a1e-fd03-4218-ef76-566dc9881e64"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|      date|sales|\n",
            "+----------+-----+\n",
            "|2023-10-30|  136|\n",
            "|2023-11-14|  223|\n",
            "|2023-11-22|  450|\n",
            "|2023-11-25|  290|\n",
            "|2023-12-19|  189|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check data type of each column\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP8xe0EdQKhB",
        "outputId": "4d64e546-4554-43e7-cae4-0f6ba125586f"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('date', 'date'), ('sales', 'bigint')]"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import date_format\n",
        "#create new column that converts dates to strings\n",
        "df_new = df.withColumn('date_string', date_format('date','MM/dd/yyyy'))\n",
        "#view new DataFrame\n",
        "df_new.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVioDWAWQRgK",
        "outputId": "896dab25-0c11-4ff9-a33b-0746ee4b4bb5"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----------+\n",
            "|      date|sales|date_string|\n",
            "+----------+-----+-----------+\n",
            "|2023-10-30|  136| 10/30/2023|\n",
            "|2023-11-14|  223| 11/14/2023|\n",
            "|2023-11-22|  450| 11/22/2023|\n",
            "|2023-11-25|  290| 11/25/2023|\n",
            "|2023-12-19|  189| 12/19/2023|\n",
            "+----------+-----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check data type of each column\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHC0WzHFQXSx",
        "outputId": "1a24fc36-7c5d-48e0-d6e9-e81ba48c8e54"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('date', 'date'), ('sales', 'bigint')]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Convert String to Date in PySpark\n",
        "(With Example)"
      ],
      "metadata": {
        "id": "JvitOnPLQbyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data = [['2023-01-15', 225],\n",
        "['2023-02-24', 260],\n",
        "['2023-07-14', 413],\n",
        "['2023-10-30', 368]]\n",
        "\n",
        "#define column names\n",
        "columns = ['date', 'sales']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQBbVrxGQZVy",
        "outputId": "845cbe88-8e90-4487-fdde-c71d7fc49b3c"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|      date|sales|\n",
            "+----------+-----+\n",
            "|2023-01-15|  225|\n",
            "|2023-02-24|  260|\n",
            "|2023-07-14|  413|\n",
            "|2023-10-30|  368|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check data type of each column\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQYWBVSGQgQc",
        "outputId": "e2cf445b-ce87-444c-ca27-48cc932641a4"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('date', 'string'), ('sales', 'bigint')]"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "#convert 'date' column from string to date\n",
        "df = df.withColumn('date', F.to_date('date'))\n",
        "\n",
        "#view updated DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqGWJx8xQkwh",
        "outputId": "356c8cf9-248e-4d18-f4e7-a04021a34ccd"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|      date|sales|\n",
            "+----------+-----+\n",
            "|2023-01-15|  225|\n",
            "|2023-02-24|  260|\n",
            "|2023-07-14|  413|\n",
            "|2023-10-30|  368|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check data type of each column\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmikSkS7QqzC",
        "outputId": "f773a219-1b31-4e1a-c066-c9ecac869adb"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('date', 'date'), ('sales', 'bigint')]"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Convert String to Timestamp in\n",
        "PySpark (With Example)"
      ],
      "metadata": {
        "id": "FZtsdbF0Q1Rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define data\n",
        "data = [['2023-01-15 04:14:22', 225],\n",
        "['2023-02-24 10:55:01', 260],\n",
        "['2023-07-14 18:34:59', 413],\n",
        "['2023-10-30 22:20:05', 368]]\n",
        "\n",
        "#define column names\n",
        "columns = ['ts', 'sales']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8GyDR5UQwua",
        "outputId": "1c9efb95-533f-4988-ebf9-c1f6410ba86d"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----+\n",
            "|                 ts|sales|\n",
            "+-------------------+-----+\n",
            "|2023-01-15 04:14:22|  225|\n",
            "|2023-02-24 10:55:01|  260|\n",
            "|2023-07-14 18:34:59|  413|\n",
            "|2023-10-30 22:20:05|  368|\n",
            "+-------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check data type of each column\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5Cz7tWqQ9Ea",
        "outputId": "5852984c-0c11-4b41-e0c4-93227e8b0ee9"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ts', 'string'), ('sales', 'bigint')]"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "#convert 'ts' column from string to timestamp\n",
        "df = df.withColumn('ts_new', F.to_timestamp('ts', 'yyyy-MM-dd HH:mm:ss'))#view updated DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_au9IK-RAFl",
        "outputId": "65fc13dd-ea7c-414b-b03b-d89bb93966f8"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----+-------------------+\n",
            "|                 ts|sales|             ts_new|\n",
            "+-------------------+-----+-------------------+\n",
            "|2023-01-15 04:14:22|  225|2023-01-15 04:14:22|\n",
            "|2023-02-24 10:55:01|  260|2023-02-24 10:55:01|\n",
            "|2023-07-14 18:34:59|  413|2023-07-14 18:34:59|\n",
            "|2023-10-30 22:20:05|  368|2023-10-30 22:20:05|\n",
            "+-------------------+-----+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check data type of each column\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoQPCp4aRAC2",
        "outputId": "ec0a1357-5794-457d-bc42-2f278bc7cac4"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ts', 'string'), ('sales', 'bigint'), ('ts_new', 'timestamp')]"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Convert Timestamp to Date in\n",
        "PySpark (With Example)"
      ],
      "metadata": {
        "id": "v0gJEj4qRQjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "#define data\n",
        "data = [['2023-01-15 04:14:22', 225],\n",
        "['2023-02-24 10:55:01', 260],\n",
        "['2023-07-14 18:34:59', 413],\n",
        "['2023-10-30 22:20:05', 368]]\n",
        "\n",
        "#define column names\n",
        "columns = ['ts', 'sales']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "#convert string column to timestamp\n",
        "df = df.withColumn('ts', F.to_timestamp('ts', 'yyyy-MM-dd HH:mm:ss'))\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM59FEIDRAAO",
        "outputId": "d5aecde4-a2ae-4cac-c0cb-012d57627f3f"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----+\n",
            "|                 ts|sales|\n",
            "+-------------------+-----+\n",
            "|2023-01-15 04:14:22|  225|\n",
            "|2023-02-24 10:55:01|  260|\n",
            "|2023-07-14 18:34:59|  413|\n",
            "|2023-10-30 22:20:05|  368|\n",
            "+-------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check data type of each column\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVHPv9F2Q_84",
        "outputId": "b6acb545-9ff9-4c15-bb24-cd617eb121d9"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ts', 'timestamp'), ('sales', 'bigint')]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DateType#create date column from timestamp column\n",
        "df = df.withColumn('new_date', df['ts'].cast(DateType()))\n",
        "#view updated DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTRSJCy7RbD7",
        "outputId": "f9f9ad73-2ea7-4d4f-fda1-31873eb5c050"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----+----------+\n",
            "|                 ts|sales|  new_date|\n",
            "+-------------------+-----+----------+\n",
            "|2023-01-15 04:14:22|  225|2023-01-15|\n",
            "|2023-02-24 10:55:01|  260|2023-02-24|\n",
            "|2023-07-14 18:34:59|  413|2023-07-14|\n",
            "|2023-10-30 22:20:05|  368|2023-10-30|\n",
            "+-------------------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Convert String to Integer in PySpark\n",
        "(With Example)"
      ],
      "metadata": {
        "id": "IVLdSOSdRlFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define data\n",
        "data = [['A', '11'],\n",
        "['B', '19'],\n",
        "['C', '22'],\n",
        "['D', '25'],\n",
        "['E', '12'],\n",
        "['F', '41'],\n",
        "['G', '32'],\n",
        "['H', '20']]\n",
        "\n",
        "#define column names\n",
        "columns = ['team', 'points']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USSbg1A0RbBW",
        "outputId": "8bf9ff01-d2b4-496d-da94-f8f87e8a76e3"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|team|points|\n",
            "+----+------+\n",
            "|   A|    11|\n",
            "|   B|    19|\n",
            "|   C|    22|\n",
            "|   D|    25|\n",
            "|   E|    12|\n",
            "|   F|    41|\n",
            "|   G|    32|\n",
            "|   H|    20|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check data type of each column\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5KXib3TRa-G",
        "outputId": "8c7fd462-0252-4887-f914-076d7ed1341e"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('team', 'string'), ('points', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "#create integer column from string column\n",
        "df = df.withColumn('points_integer',\n",
        "df['points'].cast(IntegerType()))\n",
        "\n",
        "#view updated DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crkGoCfKRa6n",
        "outputId": "a14c5cd6-8474-43d6-cfc1-a6a6fad6f9a3"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+--------------+\n",
            "|team|points|points_integer|\n",
            "+----+------+--------------+\n",
            "|   A|    11|            11|\n",
            "|   B|    19|            19|\n",
            "|   C|    22|            22|\n",
            "|   D|    25|            25|\n",
            "|   E|    12|            12|\n",
            "|   F|    41|            41|\n",
            "|   G|    32|            32|\n",
            "|   H|    20|            20|\n",
            "+----+------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check data type of each column\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwfpbZfGQ_5k",
        "outputId": "0547b949-c964-4fdd-ac48-f2a396fd01c1"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('team', 'string'), ('points', 'string'), ('points_integer', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5s8T9M6lQ_1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Convert Integer to String in PySpark\n",
        "(With Example)"
      ],
      "metadata": {
        "id": "n6JIhT7ER2z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "#define data\n",
        "data = [['A', 11],\n",
        "['B', 19],\n",
        "['C', 22],\n",
        "['D', 25],\n",
        "['E', 12],\n",
        "['F', 41],\n",
        "['G', 32],\n",
        "['H', 20]]\n",
        "\n",
        "#define column names\n",
        "columns = ['team', 'points']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCMHD-MMRCH7",
        "outputId": "f324b77d-85e4-4b15-ebb4-0c75f80d1adb"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|team|points|\n",
            "+----+------+\n",
            "|   A|    11|\n",
            "|   B|    19|\n",
            "|   C|    22|\n",
            "|   D|    25|\n",
            "|   E|    12|\n",
            "|   F|    41|\n",
            "|   G|    32|\n",
            "|   H|    20|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check data type of each column\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ptLJbUyR8lK",
        "outputId": "8ba43a38-a4b8-48be-ad90-93b690ac801c"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('team', 'string'), ('points', 'bigint')]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType\n",
        "\n",
        "#create string column from integer column\n",
        "df = df.withColumn('points_string',\n",
        "df['points'].cast(StringType()))\n",
        "\n",
        "#view updated DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH1FsHP7R_c6",
        "outputId": "aae58808-b8e2-4447-bd67-c6fb7ae56c54"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+-------------+\n",
            "|team|points|points_string|\n",
            "+----+------+-------------+\n",
            "|   A|    11|           11|\n",
            "|   B|    19|           19|\n",
            "|   C|    22|           22|\n",
            "|   D|    25|           25|\n",
            "|   E|    12|           12|\n",
            "|   F|    41|           41|\n",
            "|   G|    32|           32|\n",
            "|   H|    20|           20|\n",
            "+----+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check data type of each column\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aWZOjq7SGdK",
        "outputId": "06c118f1-ffeb-4faa-ea5a-cceea797568e"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('team', 'string'), ('points', 'bigint'), ('points_string', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xiEyqoJUSJli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark: How to Convert RDD to\n",
        "DataFrame (With Example)"
      ],
      "metadata": {
        "id": "Ij5f96FfSNTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data = [('A', 11),\n",
        "('B', 19),\n",
        "('C', 22),\n",
        "('D', 25),\n",
        "('E', 12),\n",
        "('F', 41)]\n",
        "#create RDD using data\n",
        "my_RDD = spark.sparkContext.parallelize(data)"
      ],
      "metadata": {
        "id": "i_PNrXBeSKE-"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check object type\n",
        "type(my_RDD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWE0nWLBSKCE",
        "outputId": "a7441898-a317-40d7-980e-6277fe4a192e"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert RDD to DataFrame\n",
        "my_df = my_RDD.toDF()"
      ],
      "metadata": {
        "id": "8oI1OY7oSJ_M"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#view DataFrame\n",
        "my_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYANTHN7SJ7f",
        "outputId": "8c83fb07-a718-45d4-e758-de24851eff66"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "| _1| _2|\n",
            "+---+---+\n",
            "|  A| 11|\n",
            "|  B| 19|\n",
            "|  C| 22|\n",
            "|  D| 25|\n",
            "|  E| 12|\n",
            "|  F| 41|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check object type\n",
        "type(my_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wHOAKg7SJ4R",
        "outputId": "b034ad9b-b03c-468c-97d6-dca7cd57c6d0"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert RDD to DataFrame with specific column names\n",
        "my_df = my_RDD.toDF(['player', 'assists'])\n",
        "\n",
        "#view DataFrame\n",
        "my_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkUuf_z8SetR",
        "outputId": "1126648f-6987-4151-94b4-73991754485f"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+\n",
            "|player|assists|\n",
            "+------+-------+\n",
            "|     A|     11|\n",
            "|     B|     19|\n",
            "|     C|     22|\n",
            "|     D|     25|\n",
            "|     E|     12|\n",
            "|     F|     41|\n",
            "+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uEKQ1DI9Sl35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark: How to Convert Column to\n",
        "Lowercase"
      ],
      "metadata": {
        "id": "oHpn3pWTSoTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data = [['A', 'East', 11, 4],\n",
        "['A', 'East', 8, 9],\n",
        "['A', 'East', 10, 3],\n",
        "['B', 'West', 6, 12],\n",
        "['B', 'West', 6, 4],\n",
        "['C', 'East', 5, 2]]\n",
        "\n",
        "#define column names\n",
        "columns = ['team', 'conference', 'points', 'assists']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCc33Gb_So1x",
        "outputId": "5553852f-c1fc-446b-c88d-005a6978ff83"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      East|     8|      9|\n",
            "|   A|      East|    10|      3|\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lower\n",
        "\n",
        "#convert 'conference' column to lowercase\n",
        "df = df.withColumn('conference', lower(df['conference']))\n",
        "\n",
        "#view updated DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9asI4WqSvCy",
        "outputId": "1b8c2588-6f9a-4180-b421-3fc69476414b"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      east|    11|      4|\n",
            "|   A|      east|     8|      9|\n",
            "|   A|      east|    10|      3|\n",
            "|   B|      west|     6|     12|\n",
            "|   B|      west|     6|      4|\n",
            "|   C|      east|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark: How to Convert Column to\n",
        "Uppercase"
      ],
      "metadata": {
        "id": "Fpm7vOmoS55u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define data\n",
        "data = [['A', 'East', 11, 4],\n",
        "['A', 'East', 8, 9],\n",
        "['A', 'East', 10, 3],\n",
        "['B', 'West', 6, 12],\n",
        "['B', 'West', 6, 4],\n",
        "['C', 'East', 5, 2]]\n",
        "\n",
        "#define column names\n",
        "columns = ['team', 'conference', 'points', 'assists']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBr01gLHS1mC",
        "outputId": "5c9c5a47-5e97-4fe0-da10-c2a67962adf5"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      East|     8|      9|\n",
            "|   A|      East|    10|      3|\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import upper\n",
        "\n",
        "#convert 'conference' column to uppercase\n",
        "df = df.withColumn('conference', upper(df['conference']))\n",
        "\n",
        "#view updated DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg_ifQrqTAMq",
        "outputId": "2f955202-d386-4dbb-b925-cea48e842981"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      EAST|    11|      4|\n",
            "|   A|      EAST|     8|      9|\n",
            "|   A|      EAST|    10|      3|\n",
            "|   B|      WEST|     6|     12|\n",
            "|   B|      WEST|     6|      4|\n",
            "|   C|      EAST|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Use “Is Not Null” in PySpark (With\n",
        "Examples)"
      ],
      "metadata": {
        "id": "thjDZM5ZTLKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define data\n",
        "data = [['A', 'East', 11, 4],['A', None, 8, 9],\n",
        "['A', 'East', 10, 3],\n",
        "['B', 'West', None, 12],\n",
        "['B', 'West', None, 4],\n",
        "['C', 'East', 5, 2]]\n",
        "\n",
        "#define column names\n",
        "columns = ['team', 'conference', 'points', 'assists']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-khjtmSmTH0h",
        "outputId": "c8477e9a-2323-4b80-ac2e-607a5b3da76c"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      NULL|     8|      9|\n",
            "|   A|      East|    10|      3|\n",
            "|   B|      West|  NULL|     12|\n",
            "|   B|      West|  NULL|      4|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter for rows where value is not null in 'points' column\n",
        "df.filter(df.points.isNotNull()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G9cJOXQTR8y",
        "outputId": "23a1ee7b-172d-4a07-d5cd-e49ccabac474"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      NULL|     8|      9|\n",
            "|   A|      East|    10|      3|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter for rows where value is not null in any column\n",
        "df.dropna().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_4sgcnoTWwL",
        "outputId": "fe0d55f5-957b-4766-816c-6a909123ed61"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      East|    10|      3|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Use “IS NOT IN” in PySpark (With\n",
        "Example)"
      ],
      "metadata": {
        "id": "IO3JEWINTbom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define array of values\n",
        "my_array = ['A', 'D', 'E']\n",
        "#filter DataFrame to only contain rows where 'team' is not in\n",
        "my_array\n",
        "df.filter(~df.team.isin(my_array)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNYvyf7PTZXo",
        "outputId": "e994aab4-d5ba-460d-c09c-2134e0066fa5"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   B|      West|  NULL|     12|\n",
            "|   B|      West|  NULL|      4|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data = [['A', 'East', 11, 4],\n",
        "['A', 'East', 8, 9],\n",
        "['A', 'East', 10, 3],\n",
        "['B', 'West', 6, 12],\n",
        "['B', 'West', 6, 4],\n",
        "['C', 'East', 5, 2],\n",
        "['D', 'East', 14, 2],\n",
        "['E', 'West', 25, 2]]\n",
        "#define column names\n",
        "columns = ['team', 'conference', 'points', 'assists']\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOpcgu_rTflj",
        "outputId": "dda6ca7e-f912-40a6-b95e-6e323e9a0711"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      East|     8|      9|\n",
            "|   A|      East|    10|      3|\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "|   C|      East|     5|      2|\n",
            "|   D|      East|    14|      2|\n",
            "|   E|      West|    25|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define array of values\n",
        "my_array = ['A', 'D', 'E']\n",
        "#filter DataFrame to only contain rows where 'team' is not in\n",
        "my_array\n",
        "df.filter(~df.team.isin(my_array)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4WLlaJfTihj",
        "outputId": "81b987bc-9574-44d5-e2b0-f6dcbad786ed"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Use “OR” Operator in PySpark\n",
        "(With Examples)"
      ],
      "metadata": {
        "id": "hywm_-HoTpwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data = [['A', 'East', 11, 4],\n",
        "['A', 'East', 8, 9],\n",
        "['A', 'East', 10, 3],\n",
        "['B', 'West', 6, 12],\n",
        "['B', 'West', 6, 4],\n",
        "['C', 'East', 5, 2]]\n",
        "\n",
        "\n",
        "#define column names\n",
        "columns = ['team', 'conference', 'points', 'assists']\n",
        "\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53KdlnT_TmYa",
        "outputId": "7ea3adea-a194-4858-e434-cfe050e60921"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      East|     8|      9|\n",
            "|   A|      East|    10|      3|\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter DataFrame where points is greater than 9 or team equals \"B\"\n",
        "df.filter('points>9 or team==\"B\"').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLoW9ha0TvA8",
        "outputId": "808e2fe6-9bf1-4116-b3dd-10be2cb312d2"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      East|    10|      3|\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter DataFrame where points is greater than 9 or team equals \"B\"\n",
        "df.filter((df.points>9) | (df.team==\"B\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU6X0BY7T0dh",
        "outputId": "30a7f97f-d369-4874-95e4-893aba8e2c73"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      East|    10|      3|\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Use “AND” Operator in PySpark\n",
        "(With Examples)"
      ],
      "metadata": {
        "id": "fVjg8HbQT6B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "#define data\n",
        "data = [['A', 'East', 11, 4],\n",
        "['A', 'East', 8, 9],\n",
        "['A', 'East', 10, 3],\n",
        "['B', 'West', 6, 12],\n",
        "['B', 'West', 6, 4],\n",
        "['C', 'East', 5, 2]]\n",
        "\n",
        "#define column names\n",
        "columns = ['team', 'conference', 'points', 'assists']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phv4CPVRT3fq",
        "outputId": "874de395-a1a6-47db-d514-d13a05379186"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      East|     8|      9|\n",
            "|   A|      East|    10|      3|\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter DataFrame where points is greater than 5 and conference equals \"East\"\n",
        "df.filter((df.points>5) & (df.conference==\"East\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn0RBlWXUAl6",
        "outputId": "ea3c5a5b-889e-4802-ee08-dca24dd2af8e"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      East|     8|      9|\n",
            "|   A|      East|    10|      3|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Use “Not Equal” Operator in\n",
        "PySpark (With Examples)"
      ],
      "metadata": {
        "id": "9ycFj8D5UKZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#filter DataFrame where team is not equal to 'A' and points is not equal to 5\n",
        "df.filter((df.team!='A') & (df.points!=5)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ3hzKfrUFOa",
        "outputId": "f9b55201-236d-41da-9630-4a283e220c1d"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data = [['A', 'East', 11, 4],\n",
        "['A', 'East', 8, 9],\n",
        "['A', 'East', 10, 3],\n",
        "['B', 'West', 6, 12],\n",
        "['B', 'West', 6, 4],\n",
        "['C', 'East', 5, 2]]\n",
        "\n",
        "#define column names\n",
        "columns = ['team', 'conference', 'points', 'assists']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8VgMFCVUPa1",
        "outputId": "3b6162f7-b4e6-49ad-cdb9-b04b155c1cae"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   A|      East|    11|      4|\n",
            "|   A|      East|     8|      9|\n",
            "|   A|      East|    10|      3|\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter DataFrame where team is not equal to 'A'\n",
        "df.filter(df.team!='A').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpVB_Z9IUT8S",
        "outputId": "e0bc3570-e123-42b3-9f16-34d4400c7c32"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "|   C|      East|     5|      2|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter DataFrame where team is not equal to 'A' and points is not equal to 5\n",
        "df.filter((df.team!='A') & (df.points!=5)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXO4f2JNUiBq",
        "outputId": "50ed90f2-715d-4aff-b20b-3a7b648ef757"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "|   B|      West|     6|     12|\n",
            "|   B|      West|     6|      4|\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cFMTAG4ZUlsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark: How to Use Case-Insensitive\n",
        "“Contains”"
      ],
      "metadata": {
        "id": "zpzjrXKgUp06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import upper\n",
        "#perform case-insensitive filter for rows that contain 'AVS' in team column\n",
        "df.filter(upper(df.team).contains('AVS')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGke7iGhUqn7",
        "outputId": "7ce72918-2757-460e-de9f-274b8ea87099"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+------+-------+\n",
            "|team|conference|points|assists|\n",
            "+----+----------+------+-------+\n",
            "+----+----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#define data\n",
        "data = [['Mavs', 14],\n",
        "['Nets', 22],\n",
        "['Nets', 31],\n",
        "['Cavs', 27],\n",
        "['CAVS', 26],\n",
        "['Spurs', 40],\n",
        "['mavs', 23],\n",
        "['MAVS', 17],]\n",
        "#define column names\n",
        "columns = ['team', 'points']\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4togZR_4UsTp",
        "outputId": "cd0bac3e-c40c-4e85-fa7e-7ec2262aed31"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "| team|points|\n",
            "+-----+------+\n",
            "| Mavs|    14|\n",
            "| Nets|    22|\n",
            "| Nets|    31|\n",
            "| Cavs|    27|\n",
            "| CAVS|    26|\n",
            "|Spurs|    40|\n",
            "| mavs|    23|\n",
            "| MAVS|    17|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter DataFrame where team column contains 'AVS'\n",
        "df.filter(df.team.contains('AVS')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHnZmHIiUx1R",
        "outputId": "cb70767f-c63e-4788-8bbd-f9b7089b06c3"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|team|points|\n",
            "+----+------+\n",
            "|CAVS|    26|\n",
            "|MAVS|    17|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import upper\n",
        "#perform case-insensitive filter for rows that contain 'AVS' in team column\n",
        "df.filter(upper(df.team).contains('AVS')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn8a5MdoU2os",
        "outputId": "da5a5149-42e8-4d69-e332-0edb3afa8304"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|team|points|\n",
            "+----+------+\n",
            "|Mavs|    14|\n",
            "|Cavs|    27|\n",
            "|CAVS|    26|\n",
            "|mavs|    23|\n",
            "|MAVS|    17|\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter DataFrame where team does not contain 'avs'\n",
        "df.filter(~df.team.contains('avs')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhjI5292U457",
        "outputId": "ead7b99f-71d9-4cdb-cefa-d907588ad16c"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "| team|points|\n",
            "+-----+------+\n",
            "| Nets|    22|\n",
            "| Nets|    31|\n",
            "| CAVS|    26|\n",
            "|Spurs|    40|\n",
            "| MAVS|    17|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "#define data\n",
        "data = [['Mavs', 14],\n",
        "['Nets', 22],\n",
        "['Nets', 31],\n",
        "['Cavs', 27],\n",
        "['Kings', 26],\n",
        "['Spurs', 40],\n",
        "['Lakers', 23],\n",
        "['Spurs', 17],]\n",
        "\n",
        "#define column names\n",
        "columns = ['team', 'points']\n",
        "\n",
        "#create dataframe using data and column names\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "#view dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh2EZIjlU-Bq",
        "outputId": "5a9cca8f-0604-419e-b2cd-ed746e0e0165"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+\n",
            "|  team|points|\n",
            "+------+------+\n",
            "|  Mavs|    14|\n",
            "|  Nets|    22|\n",
            "|  Nets|    31|\n",
            "|  Cavs|    27|\n",
            "| Kings|    26|\n",
            "| Spurs|    40|\n",
            "|Lakers|    23|\n",
            "| Spurs|    17|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filter DataFrame where team does not contain 'avs'\n",
        "df.filter(~df.team.contains('avs')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu1fh7pUVERy",
        "outputId": "4c43dfe7-a45a-4a2d-eac7-436279e218d2"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+\n",
            "|  team|points|\n",
            "+------+------+\n",
            "|  Nets|    22|\n",
            "|  Nets|    31|\n",
            "| Kings|    26|\n",
            "| Spurs|    40|\n",
            "|Lakers|    23|\n",
            "| Spurs|    17|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GMCFGFn5VHqB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}